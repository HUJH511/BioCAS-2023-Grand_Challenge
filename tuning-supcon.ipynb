{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Self-supervised Contrastive Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Content\n",
    "1. [Dependencies](#import-and-set-up-dependencies)\n",
    "    - [Tune Config](#tune-settings)\n",
    "    - [Base Model Config](#base-model-configuration)\n",
    "2. [Data](#prepare-datasets)\n",
    "3. [Tuning Loop](#set-up-tuning-loop)\n",
    "4. [Tune](#tune)\n",
    "5. [Logging](#log-results)\n",
    "6. [Test](#testing-using-tuned-model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Set Up Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leowc\\anaconda3\\envs\\ECGJH\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda on NVIDIA GeForce GTX 1650 with Max-Q Design :D \n"
     ]
    }
   ],
   "source": [
    "#   Setup\n",
    "##  Standard packages\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "##  Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as vtransforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torch import optim\n",
    "\n",
    "##  SRC dataset, loader, and metrics\n",
    "import src.data.dataset as ds\n",
    "import src.data.dataloader as dl\n",
    "import src.models as mdl\n",
    "import src.utils.metric as customMetric\n",
    "from src.utils.metric import calc_score\n",
    "\n",
    "##  Self-supervised Contrastive Learning\n",
    "from src.train import train_supcon, valid_supcon\n",
    "from src.utils.supcontrast import TwoCropTransform, AverageMeter, SupConLoss\n",
    "from src.utils.supcontrast import adjust_learning_rate, warmup_learning_rate\n",
    "from src.utils.supcontrast import set_optimizer, save_model\n",
    "from supCon import set_model\n",
    "from src.test import test_supcon\n",
    "\n",
    "##  Tuning Packages\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.air import session\n",
    "from ray.air.checkpoint import Checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.bayesopt import BayesOptSearch\n",
    "from ray.tune.search.hyperopt import HyperOptSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda on NVIDIA GeForce GTX 1650 with Max-Q Design :D \n"
     ]
    }
   ],
   "source": [
    "# -------------------- Globals --------------------#\n",
    "# Device Config\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {DEVICE} on {torch.cuda.get_device_name(0)} :D \")\n",
    "\n",
    "# Dataset Config\n",
    "TASK_IN = \"Task_11\"\n",
    "MAX_LENGTH = 3.0\n",
    "SR = 8000\n",
    "HOP_LENGTH = 128\n",
    "MAX_LENGTH_SAMPLES = int(MAX_LENGTH * SR / HOP_LENGTH)\n",
    "INPUT_X_DIM = int(MAX_LENGTH * SR / HOP_LENGTH)\n",
    "N_F_BIN = 64\n",
    "N_FFT = 512\n",
    "FEATURE = \"mfcc\"\n",
    "\n",
    "# DataLoader Config\n",
    "VAL_PERCENT = 0.2\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Log Config\n",
    "formatter = logging.Formatter(\"%(asctime)s:%(levelname)s:%(name)s:%(message)s\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNE_STRAT = [\"score\", \"max\"]\n",
    "BASE_CONFIG = {}\n",
    "TUNE_CONFIG = {\n",
    "    \"temperature\": tune.uniform(0.1, 0.9),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-2),\n",
    "    \n",
    "}\n",
    "TUNE_MAX_EPOCH = 20\n",
    "TUNE_GPU_PER_TRIAL = 1\n",
    "TUNE_CPU_PER_TRIAL = 8\n",
    "TUNE_SAMPLE_NUM = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_in: Task_11\n",
      "data_path: SPRSound/\n",
      "batch_size: 32\n",
      "val_percent: 0.2\n",
      "model: resnet18\n",
      "embedding_size: 128\n",
      "head: linear\n",
      "ckpt: best.pth\n",
      "print_freq: 50\n",
      "save_freq: 50\n",
      "epochs: 50\n",
      "optimizer: SGD\n",
      "learning_rate: 0.001\n",
      "momentum: 0.9\n",
      "lr_decay_rate: 0.1\n",
      "lr_decay_epochs: [70, 80, 90]\n",
      "weight_decay: 0.0001\n",
      "dropout: 0.25\n",
      "temperature: 0.5\n",
      "method: SupCon\n",
      "cosine: True\n",
      "warm: False\n",
      "verbose: False\n",
      "model_path: ./temp/SupCon-Notes/Task_11_models\n",
      "model_name: resnet18_mfcc64_linear128_hop128_SGD_lr0.001_temp0.5_drop0.25_val0.2\n",
      "save_folder: ./temp/SupCon-Notes/Task_11_models\\resnet18_mfcc64_linear128_hop128_SGD_lr0.001_temp0.5_drop0.25_val0.2\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Define customized Argparse --------------------#\n",
    "class modelSetting:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "    def print_args(self):\n",
    "        argparse_dict = vars(self)\n",
    "        for key, value in argparse_dict.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "opt = modelSetting(\n",
    "    # Dataset Config\n",
    "    task_in = \"Task_11\", data_path = \"SPRSound/\", \n",
    "    batch_size = BATCH_SIZE, val_percent = 0.2,\n",
    "    \n",
    "    # Model Config\n",
    "    model = \"resnet18\", embedding_size = 128, \n",
    "    head = \"linear\", ckpt = \"best.pth\", \n",
    "\n",
    "    # Train Config\n",
    "    print_freq = 50, save_freq = 50, epochs = 50,\n",
    "\n",
    "    # Optim Config\n",
    "    optimizer = \"SGD\",\n",
    "    learning_rate = 0.001, momentum = 0.9,\n",
    "    lr_decay_rate = 0.1, lr_decay_epochs = \"70,80,90\",\n",
    "    weight_decay = 1e-4, dropout = 0.25,\n",
    "\n",
    "    # SupCon Config\n",
    "    temperature = 0.5, method = \"SupCon\",\n",
    "\n",
    "    # Other Config\n",
    "    cosine = True, warm = False, verbose = False,\n",
    ")\n",
    "\n",
    "iterations = opt.lr_decay_epochs.split(\",\")\n",
    "opt.lr_decay_epochs = list([])\n",
    "for it in iterations:\n",
    "    opt.lr_decay_epochs.append(int(it))\n",
    "\n",
    "# warm-up for large-batch training,\n",
    "if opt.batch_size > 256:\n",
    "    opt.warm = True\n",
    "if opt.warm:\n",
    "    opt.warmup_from = 0.01\n",
    "    opt.warm_epochs = 10\n",
    "    if opt.cosine:\n",
    "        eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)\n",
    "        opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (\n",
    "                1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2\n",
    "    else:\n",
    "        opt.warmup_to = opt.learning_rate\n",
    "\n",
    "# set the path according to the environment\n",
    "opt.model_path = \"./temp/SupCon-Notes/{}_models\".format(opt.task_in)\n",
    "opt.model_name = \"{}_{}{}_{}{}_hop{}_{}_lr{}_temp{}_drop{}_val{}\".format(\n",
    "    opt.model, \n",
    "    FEATURE, \n",
    "    N_F_BIN, \n",
    "    opt.head,\n",
    "    opt.embedding_size,\n",
    "    HOP_LENGTH, \n",
    "    opt.optimizer,\n",
    "    opt.learning_rate,\n",
    "    opt.temperature,\n",
    "    opt.dropout,\n",
    "    opt.val_percent,\n",
    ")\n",
    "opt.save_folder = os.path.join(opt.model_path, opt.model_name)\n",
    "if not os.path.isdir(opt.save_folder):\n",
    "    os.makedirs(opt.save_folder)\n",
    "\n",
    "opt.print_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- User-defined functions --------------------#\n",
    "def setupLogger(name, logPath, level=logging.INFO):\n",
    "    handler = logging.FileHandler(logPath)\n",
    "    handler.setFormatter(formatter)\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    logger.addHandler(handler)\n",
    "    return logger\n",
    "\n",
    "## Set Globals\n",
    "log_path = \"logs/hyperTune.logs\"\n",
    "if not os.path.exists(log_path):\n",
    "    open(log_path, \"a\").close()\n",
    "logger = setupLogger(\"TuneSupConLogger\", log_path)\n",
    "main_task = int(TASK_IN[-2])\n",
    "sub_task = int(TASK_IN[-1])\n",
    "data_path = \"SPRSound\"\n",
    "num_classes = len(dl.classes[TASK_IN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict={\n",
    "    \"train\":[\n",
    "        os.path.join(data_path, \"train_wav\"), \n",
    "        os.path.join(data_path, \"train_json\")\n",
    "    ],\n",
    "    \"intra_test\":[\n",
    "        os.path.join(data_path, \"test_wav\"), \n",
    "        os.path.join(data_path, \"test_json/intra_test_json\")\n",
    "    ],\n",
    "    \"inter_test\":[\n",
    "        os.path.join(data_path, \"test_wav\"),\n",
    "        os.path.join(data_path, \"test_json/inter_test_json\")\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Home](#table-of-content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preapare transformation and generate dataset\n",
    "train_transform = vtransforms.Compose([\n",
    "    vtransforms.RandomHorizontalFlip(),\n",
    "    vtransforms.RandomCrop(size=(N_F_BIN, MAX_LENGTH_SAMPLES), padding=0, pad_if_needed=True),\n",
    "])\n",
    "\n",
    "trainDataset, intra_testDataset, inter_testDataset = ds.genDatasets(\n",
    "    task=main_task, \n",
    "    data_dict=data_dict,\n",
    "    resample=None,\n",
    "    feature=FEATURE,\n",
    "    pre_emph=False,\n",
    "    pos_norm=\"zscore\",\n",
    "    n_mfcc=N_F_BIN,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    n_fft=N_FFT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Data... 20% Validation Set\n",
      "\n",
      "Batch Size: 32\n",
      "\n",
      "Train Len = 5325 , Validation Len = 1331\n",
      "\n",
      "Balanced sampler is used for train.\n",
      "Number of samples in each class:\n",
      " Counter({'Normal': 4119, 'Adventitious': 1206})\n",
      "\n",
      "Balanced sampler is used for val.\n",
      "Number of samples in each class:\n",
      " Counter({'Normal': 1040, 'Adventitious': 291})\n",
      "\n",
      "Train Size Batched = 166 , Validation Size Batched = 41\n",
      "\n",
      "\n",
      "Generating Dataloader for Train Dataset...\n",
      "Getting Data... 20% Validation Set\n",
      "\n",
      "Batch Size: 32\n",
      "\n",
      "Train Len = 5325 , Validation Len = 1331\n",
      "\n",
      "Balanced sampler is used for train.\n",
      "Number of samples in each class:\n",
      " Counter({'Normal': 4122, 'Adventitious': 1203})\n",
      "\n",
      "Balanced sampler is used for val.\n",
      "Number of samples in each class:\n",
      " Counter({'Normal': 1037, 'Adventitious': 294})\n",
      "\n",
      "Train Size Batched = 166 , Validation Size Batched = 41\n",
      "\n",
      "\n",
      "Generating Dataloader for Intra Dataset...\n",
      "Batch Size: 32\n",
      "Test Len = 1004, Test Size Batched = 31\n",
      "\n",
      "Generating Dataloader for Inter Dataset...\n",
      "Batch Size: 32\n",
      "Test Len = 1429, Test Size Batched = 44\n"
     ]
    }
   ],
   "source": [
    "supcon_loader = dl.trainValLoader(\n",
    "    trainDataset,\n",
    "    sub_task,\n",
    "    valid_size=VAL_PERCENT,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=lambda batch: dl.supcon_collate(\n",
    "        batch, TASK_IN, sub_task, transform=TwoCropTransform(train_transform)\n",
    "    ),\n",
    "    train_sampler=\"balanced\",\n",
    "    val_sampler=\"balanced\",\n",
    ")\n",
    "\n",
    "print(\"\\n\\nGenerating Dataloader for Train Dataset...\")\n",
    "dataloader = dl.trainValLoader(\n",
    "    trainDataset,\n",
    "    sub_task,\n",
    "    valid_size=VAL_PERCENT,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=lambda batch: dl.custom_collate(\n",
    "        batch, MAX_LENGTH_SAMPLES, TASK_IN, sub_task\n",
    "    ),\n",
    "    train_sampler=\"balanced\",\n",
    "    val_sampler=\"balanced\",\n",
    ")\n",
    "\n",
    "print(\"\\n\\nGenerating Dataloader for Intra Dataset...\")\n",
    "intra_testloader = dl.testLoader(\n",
    "    intra_testDataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=lambda batch: dl.custom_collate(\n",
    "        batch, MAX_LENGTH_SAMPLES, TASK_IN, sub_task\n",
    "    ),\n",
    "    shuffle_in=False,\n",
    ")\n",
    "\n",
    "print(\"\\nGenerating Dataloader for Inter Dataset...\")\n",
    "inter_testloader = dl.testLoader(\n",
    "    inter_testDataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=lambda batch: dl.custom_collate(\n",
    "        batch, MAX_LENGTH_SAMPLES, TASK_IN, sub_task\n",
    "    ),\n",
    "    shuffle_in=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Home](#table-of-content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Tuning-Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingLoop(config, opt, supcon_loader, dataloader):\n",
    "    opt.learning_rate = config[\"lr\"]\n",
    "    opt.temperature = config[\"temperature\"]\n",
    "    model, criterion = set_model(opt)\n",
    "    optimizer = set_optimizer(opt, model)\n",
    "    print(\"\\n\\nTraining...\")\n",
    "    print(\"Running for {} epochs...\".format(opt.epochs))\n",
    "    best_loss = 0\n",
    "    best_epoch = 1\n",
    "    # training routine\n",
    "    for epoch in range(1, opt.epochs + 1):\n",
    "        adjust_learning_rate(opt, optimizer, epoch)\n",
    "\n",
    "        # train for one epoch\n",
    "        time1 = time.time()\n",
    "        train_loss = train_supcon(supcon_loader[\"train\"], model, criterion, optimizer, epoch, opt)\n",
    "        valid_loss = valid_supcon(supcon_loader[\"val\"], model, criterion, opt)\n",
    "        time2 = time.time()\n",
    "        print(\"epoch {}, total time {:.2f}, train loss: {:.2f}, valid loss: {:.2f}\".format(epoch, time2 - time1, train_loss, 1/valid_loss))\n",
    "\n",
    "        if valid_loss > best_loss:\n",
    "            best_loss = valid_loss\n",
    "            best_model = model\n",
    "            best_optimizer = optimizer\n",
    "            best_epoch = epoch\n",
    "    print(\"\\n\\nTesting..\")\n",
    "    best_model.eval()\n",
    "    targets = []\n",
    "    embeddings = torch.zeros((0, opt.embedding_size), dtype=torch.float32)\n",
    "    for data, label, _ in dataloader[\"train\"]:\n",
    "        data = data.to(DEVICE)\n",
    "        embedding = best_model(data)\n",
    "        targets.extend(label.detach().cpu().tolist())\n",
    "        embeddings = torch.cat((embeddings, embedding.detach().cpu()), dim=0)\n",
    "    x_embed = np.array(embeddings)\n",
    "    y = np.array(targets)\n",
    "\n",
    "    # Create a logistic regression classifier\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(x_embed, y)\n",
    "    predictions = classifier.predict(x_embed)\n",
    "\n",
    "    print(\"\\nResult for Train:\")\n",
    "    train_score, *_ = calc_score(y, predictions, verbose=True, task=int(opt.task_in[-2]))\n",
    "    print(\"\\nResult for Valid:\")\n",
    "    val_score = test_supcon(best_model, classifier, dataloader[\"val\"], opt)\n",
    "    # Here we save a checkpoint. It is automatically registered with\n",
    "    # Ray Tune and can be accessed through `session.get_checkpoint()`\n",
    "    # API in future iterations.\n",
    "    os.makedirs(\"tuning_models\", exist_ok=True)\n",
    "    torch.save(\n",
    "        (best_model.state_dict(), optimizer.state_dict()),\n",
    "        \"tuning_models/checkpoint.pt\",\n",
    "    )\n",
    "    checkpoint = Checkpoint.from_directory(\"tuning_models\")\n",
    "    session.report(\n",
    "        {\n",
    "            \"score\": val_score,\n",
    "        },\n",
    "        checkpoint=checkpoint,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_target, strat_mode = TUNE_STRAT\n",
    "scheduler = ASHAScheduler(max_t=TUNE_MAX_EPOCH, grace_period=1, reduction_factor=2)\n",
    "algo = HyperOptSearch(metric=strat_target, mode=strat_mode)\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_resources(\n",
    "        tune.with_parameters(\n",
    "            trainingLoop, opt=opt, supcon_loader=supcon_loader, dataloader=dataloader\n",
    "        ),\n",
    "        resources={\"cpu\": TUNE_CPU_PER_TRIAL, \"gpu\": TUNE_GPU_PER_TRIAL},\n",
    "    ),\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=strat_target,\n",
    "        mode=strat_mode,\n",
    "        scheduler=scheduler,\n",
    "        num_samples=TUNE_SAMPLE_NUM,\n",
    "        search_alg=algo,\n",
    "    ),\n",
    "    param_space=TUNE_CONFIG,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Home](#table-of-content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 10:54:35,824\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-05-19 16:05:27</td></tr>\n",
       "<tr><td>Running for: </td><td>05:10:47.51        </td></tr>\n",
       "<tr><td>Memory:      </td><td>11.2/15.7 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=3<br>Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.9622815461202312<br>Resources requested: 0/12 CPUs, 0/1 GPUs, 0.0/2.22 GiB heap, 0.0/1.11 GiB objects\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  temperature</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   score</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>trainingLoop_c17e11e1</td><td>TERMINATED</td><td>127.0.0.1:22992</td><td style=\"text-align: right;\">0.000629898</td><td style=\"text-align: right;\">     0.407588</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2037.44</td><td style=\"text-align: right;\">0.955615</td></tr>\n",
       "<tr><td>trainingLoop_12dd535b</td><td>TERMINATED</td><td>127.0.0.1:22992</td><td style=\"text-align: right;\">0.00155857 </td><td style=\"text-align: right;\">     0.180024</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1983.23</td><td style=\"text-align: right;\">0.957936</td></tr>\n",
       "<tr><td>trainingLoop_72ca5cc1</td><td>TERMINATED</td><td>127.0.0.1:22992</td><td style=\"text-align: right;\">0.00296015 </td><td style=\"text-align: right;\">     0.771114</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1746.91</td><td style=\"text-align: right;\">0.969826</td></tr>\n",
       "<tr><td>trainingLoop_53eada76</td><td>TERMINATED</td><td>127.0.0.1:22992</td><td style=\"text-align: right;\">0.000214106</td><td style=\"text-align: right;\">     0.869231</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1774.47</td><td style=\"text-align: right;\">0.954172</td></tr>\n",
       "<tr><td>trainingLoop_60175507</td><td>TERMINATED</td><td>127.0.0.1:22992</td><td style=\"text-align: right;\">0.00667781 </td><td style=\"text-align: right;\">     0.332267</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1691.4 </td><td style=\"text-align: right;\">0.972214</td></tr>\n",
       "<tr><td>trainingLoop_17504815</td><td>TERMINATED</td><td>127.0.0.1:22992</td><td style=\"text-align: right;\">0.000867774</td><td style=\"text-align: right;\">     0.859936</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1820.43</td><td style=\"text-align: right;\">0.967623</td></tr>\n",
       "<tr><td>trainingLoop_3a4aaebb</td><td>TERMINATED</td><td>127.0.0.1:22992</td><td style=\"text-align: right;\">0.000449841</td><td style=\"text-align: right;\">     0.778233</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1983.04</td><td style=\"text-align: right;\">0.966508</td></tr>\n",
       "<tr><td>trainingLoop_3ed1f54f</td><td>TERMINATED</td><td>127.0.0.1:22992</td><td style=\"text-align: right;\">0.00525245 </td><td style=\"text-align: right;\">     0.758034</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1995.78</td><td style=\"text-align: right;\">0.958055</td></tr>\n",
       "<tr><td>trainingLoop_de036944</td><td>TERMINATED</td><td>127.0.0.1:22992</td><td style=\"text-align: right;\">0.00077561 </td><td style=\"text-align: right;\">     0.259986</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1754.55</td><td style=\"text-align: right;\">0.970542</td></tr>\n",
       "<tr><td>trainingLoop_507c5726</td><td>TERMINATED</td><td>127.0.0.1:22992</td><td style=\"text-align: right;\">0.000210114</td><td style=\"text-align: right;\">     0.346956</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1850.33</td><td style=\"text-align: right;\">0.951079</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m Windows fatal exception: code 0xc0000139\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m Stack (most recent call first):\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"c:\\Users\\leowc\\anaconda3\\envs\\ECGJH\\lib\\ctypes\\__init__.py\", line 374 in __init__\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"c:\\Users\\leowc\\anaconda3\\envs\\ECGJH\\lib\\site-packages\\torch\\_ops.py\", line 220 in load_library\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"c:\\Users\\leowc\\anaconda3\\envs\\ECGJH\\lib\\site-packages\\torchvision\\extension.py\", line 20 in <module>\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"c:\\Users\\leowc\\anaconda3\\envs\\ECGJH\\lib\\site-packages\\torchvision\\io\\_load_gpu_decoder.py\", line 1 in <module>\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"c:\\Users\\leowc\\anaconda3\\envs\\ECGJH\\lib\\site-packages\\torchvision\\io\\__init__.py\", line 8 in <module>\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 992 in _find_and_load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"c:\\Users\\leowc\\anaconda3\\envs\\ECGJH\\lib\\site-packages\\torchvision\\datasets\\_optical_flow.py\", line 12 in <module>\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"c:\\Users\\leowc\\anaconda3\\envs\\ECGJH\\lib\\site-packages\\torchvision\\datasets\\__init__.py\", line 1 in <module>\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1078 in _handle_fromlist\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"c:\\Users\\leowc\\anaconda3\\envs\\ECGJH\\lib\\site-packages\\torchvision\\__init__.py\", line 5 in <module>\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"c:\\Users\\leowc\\Downloads\\BioCAS\\supCon.py\", line 13 in <module>\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"c:\\Users\\leowc\\anaconda3\\envs\\ECGJH\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 625 in _load_actor_class_from_gcs\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"c:\\Users\\leowc\\anaconda3\\envs\\ECGJH\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 522 in load_actor_class\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"c:\\Users\\leowc\\anaconda3\\envs\\ECGJH\\lib\\site-packages\\ray\\_private\\worker.py\", line 772 in main_loop\n",
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m   File \"c:\\Users\\leowc\\anaconda3\\envs\\ECGJH\\lib\\site-packages\\ray\\_private\\workers\\default_worker.py\", line 226 in <module>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=22992)\u001b[0m Using cuda on NVIDIA GeForce GTX 1650 with Max-Q Design :D \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Training...\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Running for 50 epochs...\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 1, total time 58.14, train loss: 23.74, valid loss: 23.83\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 2, total time 37.82, train loss: 23.41, valid loss: 23.46\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 3, total time 35.91, train loss: 23.25, valid loss: 24.01\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 4, total time 36.76, train loss: 23.08, valid loss: 23.76\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 6, total time 37.12, train loss: 22.77, valid loss: 23.12\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 7, total time 36.95, train loss: 22.80, valid loss: 23.23\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 8, total time 35.61, train loss: 22.58, valid loss: 22.56\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 9, total time 36.88, train loss: 22.59, valid loss: 23.66\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 10, total time 40.77, train loss: 22.53, valid loss: 23.69\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 11, total time 40.44, train loss: 22.51, valid loss: 23.33\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 12, total time 42.18, train loss: 22.45, valid loss: 22.35\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 13, total time 42.36, train loss: 22.37, valid loss: 22.98\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 14, total time 38.10, train loss: 22.38, valid loss: 22.44\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 15, total time 37.52, train loss: 22.39, valid loss: 23.75\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 16, total time 38.66, train loss: 22.35, valid loss: 22.46\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 18, total time 36.87, train loss: 22.34, valid loss: 22.45\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 19, total time 36.05, train loss: 22.21, valid loss: 23.84\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 20, total time 39.78, train loss: 22.25, valid loss: 22.48\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 21, total time 42.92, train loss: 22.19, valid loss: 22.36\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 22, total time 38.52, train loss: 22.07, valid loss: 24.12\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 23, total time 50.20, train loss: 22.13, valid loss: 24.19\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 24, total time 39.05, train loss: 22.12, valid loss: 22.83\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 25, total time 40.48, train loss: 22.02, valid loss: 22.57\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 26, total time 39.39, train loss: 21.92, valid loss: 23.30\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 27, total time 39.91, train loss: 21.90, valid loss: 22.89\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 28, total time 39.72, train loss: 21.96, valid loss: 22.95\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 29, total time 41.01, train loss: 21.71, valid loss: 23.23\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 31, total time 37.30, train loss: 21.72, valid loss: 22.32\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 32, total time 40.95, train loss: 21.75, valid loss: 22.08\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 33, total time 40.48, train loss: 21.79, valid loss: 22.57\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 34, total time 39.78, train loss: 21.65, valid loss: 22.47\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 35, total time 39.12, train loss: 21.56, valid loss: 22.74\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 36, total time 40.49, train loss: 21.62, valid loss: 22.69\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 37, total time 39.15, train loss: 21.60, valid loss: 22.77\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 38, total time 39.57, train loss: 21.47, valid loss: 22.49\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 39, total time 40.05, train loss: 21.43, valid loss: 22.53\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 40, total time 36.62, train loss: 21.48, valid loss: 22.26\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 41, total time 45.93, train loss: 21.36, valid loss: 22.28\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 42, total time 39.31, train loss: 21.40, valid loss: 22.16\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 43, total time 38.35, train loss: 21.35, valid loss: 22.70\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 44, total time 39.34, train loss: 21.27, valid loss: 22.35\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 45, total time 39.45, train loss: 21.20, valid loss: 22.68\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 46, total time 37.14, train loss: 21.10, valid loss: 22.53\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 47, total time 41.98, train loss: 21.10, valid loss: 22.26\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 48, total time 50.41, train loss: 21.26, valid loss: 22.79\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 49, total time 43.53, train loss: 21.22, valid loss: 22.59\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 50, total time 35.96, train loss: 21.26, valid loss: 22.81\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Testing..\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Result for Train:\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Sensitivity (SE): 0.9538\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Specificity (SP): 0.9608\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Average Score (AS): 0.9573\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Harmonic Score (HS): 0.9573\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Score: 0.9573\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Result for Valid:\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Sensitivity (SE): 0.9536\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Specificity (SP): 0.9576\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Average Score (AS): 0.9556\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Harmonic Score (HS): 0.9556\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Score: 0.9556\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>date               </th><th>done  </th><th>episodes_total  </th><th>experiment_id                   </th><th>experiment_tag                </th><th>hostname       </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip  </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">   score</th><th>should_checkpoint  </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id  </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>trainingLoop_12dd535b</td><td>2023-05-19_12-01-50</td><td>True  </td><td>                </td><td>64ba4ed4d2234032b3c268c15c829506</td><td>2_lr=0.0016,temperature=0.1800</td><td>LAPTOP-C4D49LCR</td><td style=\"text-align: right;\">                         1</td><td>127.0.0.1</td><td style=\"text-align: right;\">22992</td><td style=\"text-align: right;\">0.957936</td><td>True               </td><td style=\"text-align: right;\">             1983.23</td><td style=\"text-align: right;\">           1983.23</td><td style=\"text-align: right;\">       1983.23</td><td style=\"text-align: right;\"> 1684468910</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>12dd535b  </td><td style=\"text-align: right;\">     0.009974</td></tr>\n",
       "<tr><td>trainingLoop_c17e11e1</td><td>2023-05-19_11-28-47</td><td>True  </td><td>                </td><td>64ba4ed4d2234032b3c268c15c829506</td><td>1_lr=0.0006,temperature=0.4076</td><td>LAPTOP-C4D49LCR</td><td style=\"text-align: right;\">                         1</td><td>127.0.0.1</td><td style=\"text-align: right;\">22992</td><td style=\"text-align: right;\">0.955615</td><td>True               </td><td style=\"text-align: right;\">             2037.44</td><td style=\"text-align: right;\">           2037.44</td><td style=\"text-align: right;\">       2037.44</td><td style=\"text-align: right;\"> 1684466927</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>c17e11e1  </td><td style=\"text-align: right;\">     0.009974</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Training...\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Running for 50 epochs...\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 1, total time 37.60, train loss: 10.50, valid loss: 10.52\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 2, total time 37.05, train loss: 10.33, valid loss: 10.36\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 3, total time 38.79, train loss: 10.28, valid loss: 10.63\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 5, total time 37.43, train loss: 10.16, valid loss: 10.63\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 6, total time 42.19, train loss: 10.13, valid loss: 10.60\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 7, total time 40.94, train loss: 10.04, valid loss: 10.61\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 8, total time 38.57, train loss: 10.04, valid loss: 10.48\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 9, total time 40.92, train loss: 9.97, valid loss: 10.35\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 10, total time 37.57, train loss: 9.97, valid loss: 10.04\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 12, total time 40.82, train loss: 9.91, valid loss: 10.75\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 13, total time 39.20, train loss: 9.93, valid loss: 10.15\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 14, total time 36.46, train loss: 9.86, valid loss: 10.00\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 15, total time 36.62, train loss: 9.86, valid loss: 10.08\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 16, total time 38.70, train loss: 9.88, valid loss: 9.97\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 17, total time 36.86, train loss: 9.87, valid loss: 10.18\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 18, total time 35.96, train loss: 9.77, valid loss: 10.04\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 21, total time 43.72, train loss: 9.75, valid loss: 10.22\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 23, total time 37.95, train loss: 9.72, valid loss: 9.84\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 24, total time 37.43, train loss: 9.71, valid loss: 10.37\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 25, total time 39.90, train loss: 9.70, valid loss: 10.17\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 26, total time 42.12, train loss: 9.67, valid loss: 10.18\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 27, total time 38.03, train loss: 9.63, valid loss: 10.11\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 28, total time 36.69, train loss: 9.62, valid loss: 10.04\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 29, total time 39.88, train loss: 9.63, valid loss: 9.96\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 31, total time 35.65, train loss: 9.53, valid loss: 10.19\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 32, total time 36.40, train loss: 9.51, valid loss: 9.89\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 33, total time 38.34, train loss: 9.50, valid loss: 9.97\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 34, total time 38.60, train loss: 9.52, valid loss: 9.87\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 35, total time 37.80, train loss: 9.47, valid loss: 9.86\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 36, total time 37.18, train loss: 9.46, valid loss: 9.99\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 37, total time 36.71, train loss: 9.42, valid loss: 10.05\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 38, total time 38.09, train loss: 9.42, valid loss: 10.02\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 39, total time 37.84, train loss: 9.41, valid loss: 9.92\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 40, total time 37.53, train loss: 9.35, valid loss: 10.02\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 41, total time 36.97, train loss: 9.36, valid loss: 10.05\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 42, total time 41.13, train loss: 9.32, valid loss: 9.82\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 43, total time 39.50, train loss: 9.28, valid loss: 10.03\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 44, total time 35.26, train loss: 9.25, valid loss: 10.12\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 45, total time 43.04, train loss: 9.32, valid loss: 9.95\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 46, total time 36.98, train loss: 9.26, valid loss: 9.82\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 47, total time 37.20, train loss: 9.26, valid loss: 10.14\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 48, total time 42.28, train loss: 9.30, valid loss: 9.85\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 49, total time 40.77, train loss: 9.27, valid loss: 10.09\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 50, total time 41.34, train loss: 9.27, valid loss: 9.99\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Testing..\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Result for Train:\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Sensitivity (SE): 0.9655\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Specificity (SP): 0.9685\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Average Score (AS): 0.9670\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Harmonic Score (HS): 0.9670\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Score: 0.9670\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Result for Valid:\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Training...\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Running for 50 epochs...\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 1, total time 36.02, train loss: 44.82, valid loss: 44.56\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 2, total time 39.54, train loss: 44.49, valid loss: 44.52\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 4, total time 35.11, train loss: 44.18, valid loss: 45.54\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 5, total time 38.17, train loss: 44.07, valid loss: 45.06\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 6, total time 35.92, train loss: 43.77, valid loss: 45.71\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 7, total time 36.59, train loss: 43.63, valid loss: 45.70\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 8, total time 35.76, train loss: 43.29, valid loss: 45.78\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 10, total time 36.09, train loss: 43.04, valid loss: 45.04\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 11, total time 36.72, train loss: 42.96, valid loss: 45.39\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 12, total time 37.07, train loss: 43.01, valid loss: 45.24\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 13, total time 35.78, train loss: 42.85, valid loss: 45.38\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 14, total time 34.57, train loss: 42.65, valid loss: 43.72\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 15, total time 36.33, train loss: 42.57, valid loss: 44.38\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 16, total time 36.77, train loss: 42.34, valid loss: 43.93\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 17, total time 36.24, train loss: 42.43, valid loss: 42.84\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 18, total time 35.82, train loss: 42.36, valid loss: 44.91\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 19, total time 35.68, train loss: 42.21, valid loss: 43.50\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 20, total time 35.95, train loss: 41.99, valid loss: 42.54\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 21, total time 36.10, train loss: 42.00, valid loss: 43.65\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 22, total time 37.09, train loss: 41.84, valid loss: 45.47\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 23, total time 38.42, train loss: 41.87, valid loss: 43.34\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 24, total time 36.80, train loss: 41.80, valid loss: 44.15\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 25, total time 36.47, train loss: 41.60, valid loss: 43.01\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 26, total time 33.25, train loss: 41.68, valid loss: 43.91\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 27, total time 33.27, train loss: 41.51, valid loss: 43.45\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 28, total time 33.03, train loss: 41.53, valid loss: 42.84\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 29, total time 31.99, train loss: 41.23, valid loss: 43.15\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 30, total time 32.03, train loss: 41.22, valid loss: 42.98\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 31, total time 32.19, train loss: 41.22, valid loss: 42.90\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 32, total time 31.89, train loss: 41.02, valid loss: 42.89\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 33, total time 31.88, train loss: 41.13, valid loss: 43.50\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 34, total time 31.91, train loss: 41.09, valid loss: 43.92\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 35, total time 31.95, train loss: 40.91, valid loss: 42.78\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 36, total time 31.92, train loss: 40.70, valid loss: 42.92\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 37, total time 31.94, train loss: 40.55, valid loss: 42.80\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 38, total time 32.28, train loss: 40.51, valid loss: 42.85\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 39, total time 31.55, train loss: 40.37, valid loss: 42.07\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 40, total time 31.79, train loss: 40.44, valid loss: 43.16\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 41, total time 33.75, train loss: 40.24, valid loss: 43.36\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 42, total time 32.10, train loss: 40.14, valid loss: 42.25\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 43, total time 31.88, train loss: 39.91, valid loss: 42.58\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 44, total time 32.22, train loss: 40.09, valid loss: 42.68\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 45, total time 31.91, train loss: 40.03, valid loss: 43.11\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 46, total time 31.80, train loss: 39.83, valid loss: 43.70\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 47, total time 31.88, train loss: 39.85, valid loss: 43.21\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 48, total time 31.28, train loss: 39.83, valid loss: 42.29\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 49, total time 31.98, train loss: 39.78, valid loss: 42.84\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 50, total time 31.59, train loss: 39.75, valid loss: 43.16\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Testing..\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Result for Train:\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Sensitivity (SE): 0.9610\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Specificity (SP): 0.9767\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Average Score (AS): 0.9689\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Harmonic Score (HS): 0.9688\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Score: 0.9689\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Result for Valid:\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Training...\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Running for 50 epochs...\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 1, total time 32.50, train loss: 51.03, valid loss: 50.98\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 2, total time 30.33, train loss: 50.10, valid loss: 50.61\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 3, total time 30.60, train loss: 49.80, valid loss: 50.18\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 4, total time 31.38, train loss: 49.43, valid loss: 49.34\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 5, total time 31.37, train loss: 49.22, valid loss: 49.70\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 6, total time 31.02, train loss: 48.87, valid loss: 50.26\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 7, total time 31.16, train loss: 48.93, valid loss: 51.53\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 8, total time 31.65, train loss: 48.68, valid loss: 50.67\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 9, total time 32.97, train loss: 48.34, valid loss: 49.81\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 10, total time 31.74, train loss: 48.68, valid loss: 48.59\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 11, total time 31.21, train loss: 48.34, valid loss: 50.62\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 12, total time 31.46, train loss: 48.12, valid loss: 49.31\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 13, total time 32.81, train loss: 48.38, valid loss: 49.64\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 14, total time 35.63, train loss: 48.02, valid loss: 49.14\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 15, total time 32.75, train loss: 48.14, valid loss: 49.80\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 16, total time 34.08, train loss: 48.15, valid loss: 50.91\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 17, total time 39.04, train loss: 47.83, valid loss: 49.18\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 18, total time 49.95, train loss: 47.69, valid loss: 51.73\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 20, total time 34.22, train loss: 47.68, valid loss: 49.96\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 21, total time 35.13, train loss: 47.61, valid loss: 48.31\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 23, total time 34.60, train loss: 47.53, valid loss: 48.05\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 24, total time 34.85, train loss: 47.46, valid loss: 47.76\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 25, total time 36.84, train loss: 47.60, valid loss: 48.55\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 26, total time 37.34, train loss: 47.25, valid loss: 51.23\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 27, total time 41.19, train loss: 47.36, valid loss: 50.36\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 28, total time 37.62, train loss: 47.12, valid loss: 48.29\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 29, total time 38.27, train loss: 47.40, valid loss: 48.24\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 30, total time 33.78, train loss: 46.92, valid loss: 47.36\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 31, total time 34.75, train loss: 47.03, valid loss: 48.93\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 32, total time 32.48, train loss: 46.78, valid loss: 50.41\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 33, total time 34.14, train loss: 46.63, valid loss: 48.04\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 34, total time 33.64, train loss: 46.86, valid loss: 48.22\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 35, total time 37.16, train loss: 46.68, valid loss: 48.18\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 36, total time 43.55, train loss: 46.71, valid loss: 48.65\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 37, total time 33.64, train loss: 46.52, valid loss: 47.82\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 38, total time 35.73, train loss: 46.51, valid loss: 48.70\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 39, total time 35.43, train loss: 46.50, valid loss: 47.54\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 40, total time 32.18, train loss: 46.36, valid loss: 47.45\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 41, total time 32.98, train loss: 46.40, valid loss: 48.15\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 42, total time 32.04, train loss: 46.09, valid loss: 47.98\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 43, total time 35.34, train loss: 46.12, valid loss: 47.68\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 44, total time 32.51, train loss: 46.08, valid loss: 47.52\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 45, total time 32.70, train loss: 46.20, valid loss: 47.33\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 46, total time 34.02, train loss: 46.03, valid loss: 47.85\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 47, total time 34.26, train loss: 46.15, valid loss: 47.51\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 48, total time 33.28, train loss: 46.16, valid loss: 47.03\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 49, total time 41.19, train loss: 46.12, valid loss: 47.93\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 50, total time 35.06, train loss: 46.18, valid loss: 47.95\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Testing..\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Result for Train:\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Sensitivity (SE): 0.9663\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Specificity (SP): 0.9553\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Average Score (AS): 0.9608\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Harmonic Score (HS): 0.9608\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Score: 0.9608\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Result for Valid:\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Training...\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Running for 50 epochs...\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 1, total time 32.83, train loss: 19.33, valid loss: 19.14\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 2, total time 30.89, train loss: 19.20, valid loss: 19.63\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 3, total time 32.47, train loss: 19.06, valid loss: 19.51\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 4, total time 33.03, train loss: 18.93, valid loss: 19.58\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 6, total time 32.58, train loss: 18.75, valid loss: 19.07\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 7, total time 34.43, train loss: 18.63, valid loss: 18.65\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 8, total time 33.62, train loss: 18.54, valid loss: 19.66\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 9, total time 32.58, train loss: 18.58, valid loss: 19.23\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 10, total time 34.44, train loss: 18.54, valid loss: 19.59\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 11, total time 33.39, train loss: 18.38, valid loss: 18.93\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 13, total time 32.69, train loss: 18.26, valid loss: 18.58\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 14, total time 34.94, train loss: 18.31, valid loss: 19.66\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 15, total time 32.49, train loss: 18.26, valid loss: 18.78\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 16, total time 32.59, train loss: 18.20, valid loss: 18.44\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 17, total time 34.23, train loss: 18.16, valid loss: 18.50\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 18, total time 33.15, train loss: 18.17, valid loss: 19.50\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 19, total time 33.78, train loss: 18.13, valid loss: 18.45\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 20, total time 32.60, train loss: 18.12, valid loss: 18.44\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 21, total time 32.39, train loss: 18.16, valid loss: 19.32\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 22, total time 32.60, train loss: 18.01, valid loss: 18.58\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 23, total time 32.58, train loss: 18.06, valid loss: 18.73\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 24, total time 33.43, train loss: 18.00, valid loss: 18.37\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 25, total time 32.76, train loss: 17.95, valid loss: 18.99\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 26, total time 32.98, train loss: 17.89, valid loss: 18.41\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 27, total time 33.01, train loss: 17.87, valid loss: 19.62\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 28, total time 35.72, train loss: 17.80, valid loss: 18.29\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 29, total time 32.96, train loss: 17.75, valid loss: 18.36\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 30, total time 34.41, train loss: 17.74, valid loss: 18.62\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 31, total time 33.14, train loss: 17.60, valid loss: 18.85\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 32, total time 32.68, train loss: 17.59, valid loss: 18.66\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 33, total time 32.97, train loss: 17.57, valid loss: 18.36\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 34, total time 32.80, train loss: 17.54, valid loss: 18.43\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 35, total time 33.36, train loss: 17.41, valid loss: 18.51\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 36, total time 33.29, train loss: 17.38, valid loss: 19.22\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 37, total time 33.08, train loss: 17.42, valid loss: 18.11\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 38, total time 33.52, train loss: 17.25, valid loss: 18.70\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 39, total time 33.31, train loss: 17.30, valid loss: 18.57\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 40, total time 32.89, train loss: 17.08, valid loss: 18.45\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 41, total time 33.35, train loss: 17.07, valid loss: 18.84\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 42, total time 32.85, train loss: 16.98, valid loss: 18.79\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 43, total time 32.93, train loss: 17.03, valid loss: 18.28\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 44, total time 33.65, train loss: 16.90, valid loss: 18.36\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 46, total time 34.67, train loss: 16.91, valid loss: 18.69\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 47, total time 34.37, train loss: 16.92, valid loss: 18.84\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 48, total time 33.62, train loss: 16.84, valid loss: 18.67\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 49, total time 32.52, train loss: 16.79, valid loss: 18.84\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 50, total time 33.12, train loss: 16.82, valid loss: 18.77\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Testing..\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Result for Train:\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Sensitivity (SE): 0.9696\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Specificity (SP): 0.9806\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Average Score (AS): 0.9751\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Harmonic Score (HS): 0.9750\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Score: 0.9751\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Result for Valid:\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Training...\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Running for 50 epochs...\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 1, total time 32.95, train loss: 50.04, valid loss: 50.54\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 2, total time 30.71, train loss: 49.58, valid loss: 50.59\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 3, total time 32.80, train loss: 49.24, valid loss: 50.61\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 4, total time 33.28, train loss: 48.89, valid loss: 50.93\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 5, total time 32.64, train loss: 48.78, valid loss: 49.06\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 6, total time 32.70, train loss: 48.31, valid loss: 51.12\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 8, total time 32.70, train loss: 48.21, valid loss: 49.34\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 9, total time 32.64, train loss: 48.09, valid loss: 50.36\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 10, total time 32.20, train loss: 47.82, valid loss: 48.30\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 11, total time 32.82, train loss: 47.70, valid loss: 50.77\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 12, total time 32.31, train loss: 47.65, valid loss: 48.91\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 13, total time 33.98, train loss: 47.57, valid loss: 50.75\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 14, total time 32.33, train loss: 47.53, valid loss: 48.55\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 15, total time 32.24, train loss: 47.55, valid loss: 47.87\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 16, total time 32.18, train loss: 47.40, valid loss: 48.31\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 17, total time 32.84, train loss: 47.39, valid loss: 48.84\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 18, total time 31.97, train loss: 47.18, valid loss: 47.85\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 20, total time 32.18, train loss: 47.02, valid loss: 46.90\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 21, total time 31.99, train loss: 46.86, valid loss: 49.56\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 22, total time 32.21, train loss: 46.88, valid loss: 47.77\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 23, total time 31.92, train loss: 46.81, valid loss: 51.02\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 24, total time 32.39, train loss: 46.75, valid loss: 49.17\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 25, total time 32.11, train loss: 46.64, valid loss: 48.56\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 26, total time 32.52, train loss: 46.69, valid loss: 47.21\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 27, total time 32.15, train loss: 46.41, valid loss: 48.06\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 28, total time 32.55, train loss: 46.49, valid loss: 47.75\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 29, total time 32.98, train loss: 46.20, valid loss: 47.87\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 30, total time 32.98, train loss: 46.29, valid loss: 50.03\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 31, total time 33.32, train loss: 46.17, valid loss: 48.45\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 32, total time 44.77, train loss: 46.15, valid loss: 47.91\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 33, total time 39.11, train loss: 46.25, valid loss: 47.26\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 34, total time 37.26, train loss: 46.06, valid loss: 47.69\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 35, total time 40.00, train loss: 45.84, valid loss: 47.19\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 36, total time 43.69, train loss: 45.73, valid loss: 48.07\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 37, total time 43.99, train loss: 45.74, valid loss: 47.63\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 38, total time 39.28, train loss: 45.56, valid loss: 47.53\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 39, total time 39.34, train loss: 45.41, valid loss: 47.22\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 40, total time 43.30, train loss: 45.42, valid loss: 47.64\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 41, total time 39.87, train loss: 45.44, valid loss: 46.84\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 42, total time 37.06, train loss: 45.15, valid loss: 46.71\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 43, total time 41.70, train loss: 45.35, valid loss: 46.80\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 44, total time 41.17, train loss: 45.18, valid loss: 47.54\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 45, total time 46.15, train loss: 45.04, valid loss: 47.21\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 46, total time 42.31, train loss: 44.93, valid loss: 47.06\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 47, total time 37.67, train loss: 44.89, valid loss: 47.24\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 48, total time 39.86, train loss: 45.01, valid loss: 47.65\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 49, total time 40.91, train loss: 45.04, valid loss: 47.34\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 50, total time 39.17, train loss: 44.97, valid loss: 46.94\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Testing..\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Result for Train:\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Sensitivity (SE): 0.9607\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Specificity (SP): 0.9697\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Average Score (AS): 0.9652\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Harmonic Score (HS): 0.9652\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Score: 0.9652\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Result for Valid:\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Training...\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Running for 50 epochs...\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 1, total time 37.91, train loss: 45.28, valid loss: 45.08\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 2, total time 38.31, train loss: 44.78, valid loss: 45.86\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 3, total time 38.24, train loss: 44.53, valid loss: 45.94\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 4, total time 41.20, train loss: 44.45, valid loss: 46.06\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 5, total time 40.19, train loss: 43.99, valid loss: 43.78\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 6, total time 36.97, train loss: 43.96, valid loss: 46.14\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 7, total time 36.66, train loss: 43.83, valid loss: 45.51\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 8, total time 41.06, train loss: 43.63, valid loss: 45.99\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 9, total time 40.89, train loss: 43.48, valid loss: 45.66\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 10, total time 38.88, train loss: 43.36, valid loss: 45.71\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 11, total time 39.54, train loss: 43.32, valid loss: 46.20\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 12, total time 43.08, train loss: 43.25, valid loss: 46.03\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 13, total time 40.43, train loss: 43.09, valid loss: 44.86\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 14, total time 36.67, train loss: 42.88, valid loss: 44.98\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 15, total time 38.02, train loss: 42.94, valid loss: 44.21\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 16, total time 39.69, train loss: 42.90, valid loss: 45.56\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 17, total time 38.56, train loss: 42.72, valid loss: 43.67\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 18, total time 36.78, train loss: 42.75, valid loss: 43.04\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 19, total time 37.32, train loss: 42.70, valid loss: 46.05\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 20, total time 41.94, train loss: 42.64, valid loss: 44.18\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 21, total time 42.38, train loss: 42.51, valid loss: 43.45\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 22, total time 39.63, train loss: 42.50, valid loss: 44.68\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 24, total time 43.24, train loss: 42.36, valid loss: 44.50\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 25, total time 38.05, train loss: 42.25, valid loss: 45.60\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 26, total time 39.39, train loss: 42.32, valid loss: 43.16\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 27, total time 39.07, train loss: 42.28, valid loss: 42.94\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 28, total time 39.83, train loss: 42.04, valid loss: 43.42\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 29, total time 38.22, train loss: 42.16, valid loss: 43.61\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 30, total time 37.74, train loss: 41.93, valid loss: 44.17\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 31, total time 38.26, train loss: 41.86, valid loss: 45.13\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 32, total time 38.20, train loss: 41.91, valid loss: 44.00\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 33, total time 38.01, train loss: 41.84, valid loss: 45.90\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 34, total time 37.37, train loss: 41.63, valid loss: 43.17\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 35, total time 38.46, train loss: 41.73, valid loss: 42.77\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 36, total time 40.64, train loss: 41.75, valid loss: 42.93\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 37, total time 39.50, train loss: 41.61, valid loss: 43.34\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 38, total time 37.03, train loss: 41.57, valid loss: 44.38\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 39, total time 36.48, train loss: 41.27, valid loss: 43.63\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 40, total time 40.45, train loss: 41.26, valid loss: 43.56\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 41, total time 43.90, train loss: 41.27, valid loss: 43.17\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 42, total time 36.01, train loss: 41.01, valid loss: 43.14\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 43, total time 36.40, train loss: 40.98, valid loss: 42.68\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 44, total time 38.05, train loss: 41.12, valid loss: 43.26\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 45, total time 41.27, train loss: 41.00, valid loss: 42.60\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 46, total time 39.25, train loss: 40.94, valid loss: 42.81\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 47, total time 35.87, train loss: 41.14, valid loss: 42.45\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 48, total time 36.63, train loss: 40.92, valid loss: 43.32\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 50, total time 38.66, train loss: 40.87, valid loss: 42.83\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Testing..\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Sensitivity (SE): 0.9534\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Specificity (SP): 0.9798\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Average Score (AS): 0.9666\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Harmonic Score (HS): 0.9664\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Score: 0.9665\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Training...\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Running for 50 epochs...\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 1, total time 34.69, train loss: 44.05, valid loss: 44.42\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 2, total time 34.13, train loss: 43.88, valid loss: 44.63\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 3, total time 36.20, train loss: 43.68, valid loss: 44.62\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 4, total time 39.48, train loss: 43.43, valid loss: 44.05\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 5, total time 38.76, train loss: 43.14, valid loss: 44.79\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 6, total time 37.44, train loss: 42.92, valid loss: 44.94\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 7, total time 35.24, train loss: 42.90, valid loss: 43.12\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 8, total time 36.59, train loss: 42.71, valid loss: 42.75\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 9, total time 40.23, train loss: 42.38, valid loss: 44.97\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 10, total time 38.90, train loss: 42.37, valid loss: 43.71\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 11, total time 35.23, train loss: 42.33, valid loss: 44.77\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 12, total time 35.76, train loss: 42.07, valid loss: 44.65\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 13, total time 38.77, train loss: 42.01, valid loss: 42.74\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 14, total time 40.63, train loss: 41.90, valid loss: 43.16\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 15, total time 36.88, train loss: 41.73, valid loss: 43.66\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 16, total time 34.70, train loss: 41.91, valid loss: 45.32\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 17, total time 36.82, train loss: 41.63, valid loss: 43.64\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 18, total time 39.52, train loss: 41.85, valid loss: 43.27\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 19, total time 38.76, train loss: 41.53, valid loss: 44.52\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 20, total time 35.77, train loss: 41.56, valid loss: 42.56\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 21, total time 36.10, train loss: 41.39, valid loss: 41.98\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 22, total time 41.51, train loss: 41.38, valid loss: 43.08\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 23, total time 37.04, train loss: 41.25, valid loss: 41.63\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 24, total time 40.47, train loss: 41.17, valid loss: 42.26\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 25, total time 40.52, train loss: 40.99, valid loss: 42.41\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 26, total time 41.37, train loss: 40.85, valid loss: 43.21\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 27, total time 42.44, train loss: 40.87, valid loss: 42.87\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 28, total time 41.29, train loss: 40.93, valid loss: 41.90\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 29, total time 44.03, train loss: 40.78, valid loss: 41.80\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 30, total time 42.42, train loss: 40.64, valid loss: 42.20\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 31, total time 41.86, train loss: 40.63, valid loss: 43.20\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 32, total time 50.65, train loss: 40.50, valid loss: 42.72\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 33, total time 40.32, train loss: 40.62, valid loss: 41.62\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 34, total time 44.71, train loss: 40.36, valid loss: 42.42\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 35, total time 37.97, train loss: 40.33, valid loss: 42.35\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 36, total time 37.50, train loss: 40.10, valid loss: 41.83\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 37, total time 39.06, train loss: 40.24, valid loss: 42.54\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 38, total time 37.81, train loss: 40.02, valid loss: 41.50\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 39, total time 43.05, train loss: 40.05, valid loss: 42.16\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 41, total time 37.17, train loss: 39.80, valid loss: 42.34\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 42, total time 37.00, train loss: 39.60, valid loss: 42.39\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 43, total time 36.78, train loss: 39.67, valid loss: 42.17\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 44, total time 36.84, train loss: 39.62, valid loss: 42.13\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 45, total time 42.93, train loss: 39.53, valid loss: 42.33\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 46, total time 35.26, train loss: 39.47, valid loss: 42.35\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 47, total time 55.03, train loss: 39.18, valid loss: 42.41\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 48, total time 38.54, train loss: 39.19, valid loss: 42.48\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 49, total time 34.75, train loss: 39.34, valid loss: 42.33\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 50, total time 35.78, train loss: 39.28, valid loss: 42.04\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Testing..\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Result for Train:\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Sensitivity (SE): 0.9639\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Specificity (SP): 0.9707\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Average Score (AS): 0.9673\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Harmonic Score (HS): 0.9673\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Score: 0.9673\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Result for Valid:\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Sensitivity (SE): 0.9468\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Specificity (SP): 0.9694\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Average Score (AS): 0.9581\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Harmonic Score (HS): 0.9580\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Score: 0.9581\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Training...\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Running for 50 epochs...\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 1, total time 40.67, train loss: 15.14, valid loss: 15.02\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 2, total time 33.88, train loss: 14.91, valid loss: 15.22\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 3, total time 35.34, train loss: 14.81, valid loss: 15.37\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 4, total time 35.43, train loss: 14.76, valid loss: 15.39\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 5, total time 33.88, train loss: 14.69, valid loss: 15.47\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 6, total time 34.22, train loss: 14.61, valid loss: 14.89\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 7, total time 34.71, train loss: 14.53, valid loss: 14.69\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 8, total time 34.42, train loss: 14.42, valid loss: 15.20\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 9, total time 33.96, train loss: 14.39, valid loss: 14.40\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 10, total time 35.20, train loss: 14.34, valid loss: 15.36\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 11, total time 33.71, train loss: 14.36, valid loss: 14.62\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 12, total time 33.99, train loss: 14.31, valid loss: 15.51\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 13, total time 35.44, train loss: 14.32, valid loss: 14.46\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 14, total time 33.83, train loss: 14.29, valid loss: 15.33\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 15, total time 34.44, train loss: 14.30, valid loss: 14.73\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 16, total time 34.17, train loss: 14.16, valid loss: 14.23\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 17, total time 34.16, train loss: 14.23, valid loss: 15.19\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 18, total time 36.32, train loss: 14.16, valid loss: 14.16\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 19, total time 33.89, train loss: 14.07, valid loss: 14.84\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 21, total time 32.99, train loss: 14.01, valid loss: 14.28\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 23, total time 32.85, train loss: 14.00, valid loss: 14.75\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 25, total time 32.35, train loss: 13.99, valid loss: 14.31\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 26, total time 32.46, train loss: 13.91, valid loss: 15.00\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 27, total time 32.83, train loss: 13.88, valid loss: 14.45\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 28, total time 32.78, train loss: 13.86, valid loss: 15.12\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 29, total time 32.93, train loss: 13.86, valid loss: 15.12\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 30, total time 32.42, train loss: 13.84, valid loss: 14.31\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 31, total time 32.91, train loss: 13.79, valid loss: 15.26\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 32, total time 32.76, train loss: 13.78, valid loss: 14.29\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 33, total time 32.55, train loss: 13.76, valid loss: 14.39\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 34, total time 32.52, train loss: 13.70, valid loss: 14.44\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 35, total time 32.91, train loss: 13.70, valid loss: 14.32\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 36, total time 35.31, train loss: 13.65, valid loss: 14.49\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 37, total time 31.80, train loss: 13.61, valid loss: 14.52\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 38, total time 32.82, train loss: 13.62, valid loss: 14.27\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 41, total time 34.31, train loss: 13.57, valid loss: 14.63\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 42, total time 35.78, train loss: 13.50, valid loss: 14.46\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 43, total time 35.28, train loss: 13.50, valid loss: 14.19\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 44, total time 35.77, train loss: 13.53, valid loss: 14.55\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 45, total time 36.11, train loss: 13.46, valid loss: 14.37\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 46, total time 35.03, train loss: 13.45, valid loss: 14.61\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 47, total time 36.39, train loss: 13.39, valid loss: 14.47\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 48, total time 36.53, train loss: 13.44, valid loss: 14.43\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 49, total time 36.41, train loss: 13.43, valid loss: 14.69\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 50, total time 36.14, train loss: 13.42, valid loss: 14.30\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Testing..\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Result for Train:\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Sensitivity (SE): 0.9633\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Specificity (SP): 0.9612\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Average Score (AS): 0.9623\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Harmonic Score (HS): 0.9623\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Score: 0.9623\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Result for Valid:\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Sensitivity (SE): 0.9646\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Specificity (SP): 0.9765\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Average Score (AS): 0.9706\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Harmonic Score (HS): 0.9705\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Score: 0.9705\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Training...\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Running for 50 epochs...\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 1, total time 50.34, train loss: 20.35, valid loss: 20.25\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 2, total time 48.10, train loss: 20.00, valid loss: 19.91\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 3, total time 41.70, train loss: 19.89, valid loss: 20.35\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 4, total time 34.60, train loss: 19.78, valid loss: 19.84\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 5, total time 35.06, train loss: 19.60, valid loss: 19.74\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 6, total time 35.05, train loss: 19.59, valid loss: 20.47\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 7, total time 34.25, train loss: 19.58, valid loss: 20.49\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 8, total time 34.44, train loss: 19.43, valid loss: 19.93\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 9, total time 36.94, train loss: 19.35, valid loss: 20.31\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 10, total time 35.90, train loss: 19.41, valid loss: 19.95\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 11, total time 37.72, train loss: 19.19, valid loss: 20.15\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 12, total time 36.93, train loss: 19.30, valid loss: 19.79\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 13, total time 36.85, train loss: 19.23, valid loss: 20.18\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 14, total time 40.76, train loss: 19.17, valid loss: 19.95\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 15, total time 35.37, train loss: 19.14, valid loss: 19.46\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 16, total time 35.21, train loss: 18.99, valid loss: 20.98\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 17, total time 34.85, train loss: 19.03, valid loss: 19.38\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 18, total time 40.10, train loss: 18.98, valid loss: 19.24\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 19, total time 36.00, train loss: 18.91, valid loss: 20.10\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 20, total time 39.32, train loss: 18.85, valid loss: 20.49\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 21, total time 35.61, train loss: 18.98, valid loss: 19.30\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 22, total time 35.41, train loss: 18.88, valid loss: 19.39\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 23, total time 35.93, train loss: 18.83, valid loss: 20.35\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 24, total time 35.88, train loss: 18.77, valid loss: 19.28\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 25, total time 37.73, train loss: 18.82, valid loss: 19.18\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 26, total time 48.00, train loss: 18.78, valid loss: 19.21\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 27, total time 33.67, train loss: 18.72, valid loss: 19.67\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 28, total time 43.64, train loss: 18.66, valid loss: 20.78\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 29, total time 35.67, train loss: 18.66, valid loss: 19.25\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 30, total time 34.47, train loss: 18.75, valid loss: 18.97\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 31, total time 33.55, train loss: 18.54, valid loss: 20.15\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 32, total time 33.74, train loss: 18.52, valid loss: 19.27\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 33, total time 35.66, train loss: 18.50, valid loss: 18.88\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 35, total time 34.24, train loss: 18.61, valid loss: 19.42\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 36, total time 34.26, train loss: 18.41, valid loss: 18.97\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 37, total time 34.05, train loss: 18.54, valid loss: 19.17\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 38, total time 34.18, train loss: 18.38, valid loss: 18.90\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 39, total time 33.98, train loss: 18.36, valid loss: 19.54\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 40, total time 33.67, train loss: 18.43, valid loss: 19.34\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 42, total time 34.23, train loss: 18.42, valid loss: 18.87\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 43, total time 33.40, train loss: 18.30, valid loss: 19.21\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 44, total time 33.73, train loss: 18.39, valid loss: 19.25\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 45, total time 33.82, train loss: 18.27, valid loss: 18.92\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 46, total time 33.53, train loss: 18.31, valid loss: 18.97\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 47, total time 33.76, train loss: 18.20, valid loss: 19.11\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 48, total time 33.85, train loss: 18.28, valid loss: 19.24\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 49, total time 33.43, train loss: 18.29, valid loss: 19.14\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m epoch 50, total time 34.14, train loss: 18.31, valid loss: 19.16\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Testing..\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Result for Train:\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Sensitivity (SE): 0.9479\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Specificity (SP): 0.9495\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Average Score (AS): 0.9487\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Harmonic Score (HS): 0.9487\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Score: 0.9487\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m \n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Result for Valid:\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Sensitivity (SE): 0.9545\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Specificity (SP): 0.9477\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Average Score (AS): 0.9511\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Harmonic Score (HS): 0.9511\n",
      "\u001b[2m\u001b[36m(trainingLoop pid=22992)\u001b[0m Score: 0.9511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 16:05:27,898\tINFO tune.py:798 -- Total run time: 18647.60 seconds (18647.48 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "results = tuner.fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Home](#table-of-content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'temperature': 0.33226734021311427, 'lr': 0.006677813948545732}\n",
      "Best trial final validation score: 0.9722136368277405\n"
     ]
    }
   ],
   "source": [
    "best_result = results.get_best_result(strat_target, strat_mode)\n",
    "best_val_score = best_result.metrics[\"score\"]\n",
    "# best_val_loss = best_result.metrics[\"loss\"]\n",
    "# best_val_accu = best_result.metrics[\"accuracy\"]\n",
    "print(\"Best trial config: {}\".format(best_result.config))\n",
    "# print(\"Best trial final validation loss: {}\".format(best_val_loss))\n",
    "# print(\"Best trial final validation accuracy: {}\".format(best_val_accu))\n",
    "print(\"Best trial final validation score: {}\".format(best_val_score))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Home](#table-of-content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing using Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result for Intra:\n",
      "Sensitivity (SE): 0.8639\n",
      "Specificity (SP): 0.8968\n",
      "Average Score (AS): 0.8804\n",
      "Harmonic Score (HS): 0.8801\n",
      "Score: 0.8802\n",
      "\n",
      "Result for Inter:\n",
      "Sensitivity (SE): 0.9152\n",
      "Specificity (SP): 0.8702\n",
      "Average Score (AS): 0.8927\n",
      "Harmonic Score (HS): 0.8921\n",
      "Score: 0.8924\n"
     ]
    }
   ],
   "source": [
    "## Test the best Network\n",
    "### Set the opt based on best result ----- Edit this part for different variables\n",
    "opt.temperature = best_result.config[\"temperature\"]\n",
    "opt.learning_rate = best_result.config[\"lr\"]\n",
    "### --------------------------------------------------\n",
    "test_network = mdl.SupConResNet(\n",
    "    name=opt.model, \n",
    "    head=opt.head,\n",
    "    feat_dim=opt.embedding_size,\n",
    "    dropout=opt.dropout,\n",
    ").to(DEVICE)\n",
    "best_chkpt = os.path.join(best_result.checkpoint.to_directory(), \"checkpoint.pt\")\n",
    "\n",
    "model_state, _ = torch.load(best_chkpt)\n",
    "test_network.load_state_dict(model_state)\n",
    "test_network.eval()\n",
    "\n",
    "targets = []\n",
    "embeddings = torch.zeros((0, opt.embedding_size), dtype=torch.float32)\n",
    "for data, label, _ in dataloader[\"train\"]:\n",
    "    data = data.to(DEVICE)\n",
    "    embedding = test_network(data)\n",
    "    targets.extend(label.detach().cpu().tolist())\n",
    "    embeddings = torch.cat((embeddings, embedding.detach().cpu()), dim=0)\n",
    "\n",
    "x_embed = np.array(embeddings)\n",
    "y = np.array(targets)\n",
    "\n",
    "# Create a logistic regression classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(x_embed, y)\n",
    "predictions = classifier.predict(x_embed)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"\\nResult for Intra:\")\n",
    "    intra_score = test_supcon(test_network, classifier, intra_testloader, opt)\n",
    "    print(\"\\nResult for Inter:\")\n",
    "    inter_score = test_supcon(test_network, classifier, inter_testloader, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "logMessage = (\n",
    "    f\"SupContrast with: {opt.model}, Task: {TASK_IN}, inter score: {inter_score:>0.3}, \"\n",
    "    f\"intra score: {intra_score:>0.3}, val_score: {best_val_score:>0.3}, best trial config: {best_result.config}\")\n",
    "logger.info(logMessage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECGJH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
