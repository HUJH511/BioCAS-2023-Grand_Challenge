{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Self-supervised Contrastive Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Content\n",
    "1. [Dependencies](#import-and-set-up-dependencies)\n",
    "    - [Tune Config](#tune-settings)\n",
    "    - [Base Model Config](#base-model-configuration)\n",
    "2. [Data](#prepare-datasets)\n",
    "3. [Tuning Loop](#set-up-tuning-loop)\n",
    "4. [Tune](#tune)\n",
    "5. [Logging](#log-results)\n",
    "6. [Test](#testing-using-tuned-model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Set Up Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Setup\n",
    "##  Standard packages\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "##  Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as vtransforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torch import optim\n",
    "\n",
    "##  SRC dataset, loader, and metrics\n",
    "import src.data.dataset as ds\n",
    "import src.data.dataloader as dl\n",
    "import src.models as mdl\n",
    "import src.utils.metric as customMetric\n",
    "from src.utils.metric import calc_score\n",
    "\n",
    "##  Self-supervised Contrastive Learning\n",
    "from src.train import train_supcon, valid_supcon\n",
    "from src.utils.supcontrast import TwoCropTransform, AverageMeter, SupConLoss\n",
    "from src.utils.supcontrast import adjust_learning_rate, warmup_learning_rate\n",
    "from src.utils.supcontrast import set_optimizer, save_model\n",
    "from src.test import test_supcon\n",
    "\n",
    "##  Tuning Packages\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.air import session\n",
    "from ray.air.checkpoint import Checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.bayesopt import BayesOptSearch\n",
    "from ray.tune.search.hyperopt import HyperOptSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Globals --------------------#\n",
    "# Device Config\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {DEVICE} on {torch.cuda.get_device_name(0)} :D \")\n",
    "\n",
    "# Dataset Config\n",
    "TASK_IN = \"Task_11\"\n",
    "MAX_LENGTH = 3.0\n",
    "SR = 8000\n",
    "HOP_LENGTH = 128\n",
    "MAX_LENGTH_SAMPLES = int(MAX_LENGTH * SR / HOP_LENGTH)\n",
    "INPUT_X_DIM = int(MAX_LENGTH * SR / HOP_LENGTH)\n",
    "N_F_BIN = 64\n",
    "N_FFT = 512\n",
    "FEATURE = \"mfcc\"\n",
    "\n",
    "# DataLoader Config\n",
    "VAL_PERCENT = 0.2\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Log Config\n",
    "formatter = logging.Formatter(\"%(asctime)s:%(levelname)s:%(name)s:%(message)s\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNE_STRAT = [\"score\", \"max\"]\n",
    "BASE_CONFIG = {}\n",
    "TUNE_CONFIG = {\n",
    "    \"temperature\": tune.uniform(0.1, 0.9),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-2),\n",
    "    \n",
    "}\n",
    "TUNE_MAX_EPOCH = 20\n",
    "TUNE_GPU_PER_TRIAL = 1\n",
    "TUNE_CPU_PER_TRIAL = 8\n",
    "TUNE_SAMPLE_NUM = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Define customized Argparse --------------------#\n",
    "class modelSetting:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "    def print_args(self):\n",
    "        argparse_dict = vars(self)\n",
    "        for key, value in argparse_dict.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "opt = modelSetting(\n",
    "    # Dataset Config\n",
    "    task_in = \"Task_11\", data_path = \"SPRSound/\", \n",
    "    batch_size = BATCH_SIZE, val_percent = 0.2,\n",
    "    \n",
    "    # Model Config\n",
    "    model = \"resnet18\", embedding_size = 128, \n",
    "    head = \"linear\", ckpt = \"best.pth\", \n",
    "\n",
    "    # Train Config\n",
    "    print_freq = 50, save_freq = 50, epochs = 50,\n",
    "\n",
    "    # Optim Config\n",
    "    optimizer = \"SGD\",\n",
    "    learning_rate = 0.001, momentum = 0.9,\n",
    "    lr_decay_rate = 0.1, lr_decay_epochs = \"70,80,90\",\n",
    "    weight_decay = 1e-4, dropout = 0.25,\n",
    "\n",
    "    # SupCon Config\n",
    "    temperature = 0.5, method = \"SupCon\",\n",
    "\n",
    "    # Other Config\n",
    "    cosine = True, warm = False, verbose = False,\n",
    ")\n",
    "\n",
    "iterations = opt.lr_decay_epochs.split(\",\")\n",
    "opt.lr_decay_epochs = list([])\n",
    "for it in iterations:\n",
    "    opt.lr_decay_epochs.append(int(it))\n",
    "\n",
    "# warm-up for large-batch training,\n",
    "if opt.batch_size > 256:\n",
    "    opt.warm = True\n",
    "if opt.warm:\n",
    "    opt.warmup_from = 0.01\n",
    "    opt.warm_epochs = 10\n",
    "    if opt.cosine:\n",
    "        eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)\n",
    "        opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (\n",
    "                1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2\n",
    "    else:\n",
    "        opt.warmup_to = opt.learning_rate\n",
    "\n",
    "# set the path according to the environment\n",
    "opt.model_path = \"./temp/SupCon-Notes/{}_models\".format(opt.task_in)\n",
    "opt.model_name = \"{}_{}{}_{}{}_hop{}_{}_lr{}_temp{}_drop{}_val{}\".format(\n",
    "    opt.model, \n",
    "    FEATURE, \n",
    "    N_F_BIN, \n",
    "    opt.head,\n",
    "    opt.embedding_size,\n",
    "    HOP_LENGTH, \n",
    "    opt.optimizer,\n",
    "    opt.learning_rate,\n",
    "    opt.temperature,\n",
    "    opt.dropout,\n",
    "    opt.val_percent,\n",
    ")\n",
    "opt.save_folder = os.path.join(opt.model_path, opt.model_name)\n",
    "if not os.path.isdir(opt.save_folder):\n",
    "    os.makedirs(opt.save_folder)\n",
    "\n",
    "opt.print_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- User-defined functions --------------------#\n",
    "def setupLogger(name, logPath, level=logging.INFO):\n",
    "    handler = logging.FileHandler(logPath)\n",
    "    handler.setFormatter(formatter)\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    logger.addHandler(handler)\n",
    "    return logger\n",
    "\n",
    "## Set Globals\n",
    "log_path = \"logs/HyperTune.log\"\n",
    "if not os.path.exists(log_path):\n",
    "    open(log_path, \"a\").close()\n",
    "logger = setupLogger(\"TuneSupConLogger\", log_path)\n",
    "main_task = int(TASK_IN[-2])\n",
    "sub_task = int(TASK_IN[-1])\n",
    "data_path = \"SPRSound\"\n",
    "num_classes = len(dl.classes[TASK_IN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict={\n",
    "    \"train\":[\n",
    "        os.path.join(data_path, \"train_wav\"), \n",
    "        os.path.join(data_path, \"train_json\")\n",
    "    ],\n",
    "    \"intra_test\":[\n",
    "        os.path.join(data_path, \"test_wav\"), \n",
    "        os.path.join(data_path, \"test_json/intra_test_json\")\n",
    "    ],\n",
    "    \"inter_test\":[\n",
    "        os.path.join(data_path, \"test_wav\"),\n",
    "        os.path.join(data_path, \"test_json/inter_test_json\")\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Home](#table-of-content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preapare transformation and generate dataset\n",
    "train_transform = vtransforms.Compose([\n",
    "    vtransforms.RandomHorizontalFlip(),\n",
    "    vtransforms.RandomCrop(size=(N_F_BIN, MAX_LENGTH_SAMPLES), padding=0, pad_if_needed=True),\n",
    "])\n",
    "\n",
    "trainDataset, intra_testDataset, inter_testDataset = ds.genDatasets(\n",
    "    task=main_task, \n",
    "    data_dict=data_dict,\n",
    "    resample=None,\n",
    "    feature=FEATURE,\n",
    "    pre_emph=False,\n",
    "    pos_norm=\"zscore\",\n",
    "    n_mfcc=N_F_BIN,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    n_fft=N_FFT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supcon_loader = dl.trainValLoader(\n",
    "    trainDataset,\n",
    "    sub_task,\n",
    "    valid_size=VAL_PERCENT,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=lambda batch: dl.supcon_collate(\n",
    "        batch, TASK_IN, sub_task, transform=TwoCropTransform(train_transform)\n",
    "    ),\n",
    "    train_sampler=\"balanced\",\n",
    "    val_sampler=\"balanced\",\n",
    ")\n",
    "\n",
    "print(\"\\n\\nGenerating Dataloader for Train Dataset...\")\n",
    "dataloader = dl.trainValLoader(\n",
    "    trainDataset,\n",
    "    sub_task,\n",
    "    valid_size=VAL_PERCENT,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=lambda batch: dl.custom_collate(\n",
    "        batch, MAX_LENGTH_SAMPLES, TASK_IN, sub_task\n",
    "    ),\n",
    "    train_sampler=\"balanced\",\n",
    "    val_sampler=\"balanced\",\n",
    ")\n",
    "\n",
    "print(\"\\n\\nGenerating Dataloader for Intra Dataset...\")\n",
    "intra_testloader = dl.testLoader(\n",
    "    intra_testDataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=lambda batch: dl.custom_collate(\n",
    "        batch, MAX_LENGTH_SAMPLES, TASK_IN, sub_task\n",
    "    ),\n",
    "    shuffle_in=False,\n",
    ")\n",
    "\n",
    "print(\"\\nGenerating Dataloader for Inter Dataset...\")\n",
    "inter_testloader = dl.testLoader(\n",
    "    inter_testDataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=lambda batch: dl.custom_collate(\n",
    "        batch, MAX_LENGTH_SAMPLES, TASK_IN, sub_task\n",
    "    ),\n",
    "    shuffle_in=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Home](#table-of-content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Tuning-Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model(opt):\n",
    "    model = mdl.SupConResNet(\n",
    "        name=opt.model, \n",
    "        head=opt.head,\n",
    "        feat_dim=opt.embedding_size,\n",
    "        dropout=opt.dropout,\n",
    "    )\n",
    "    criterion = SupConLoss(temperature=opt.temperature)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model.encoder = torch.nn.DataParallel(model.encoder)\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "        cudnn.benchmark = True\n",
    "    \n",
    "    return model, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingLoop(config, opt, supcon_loader, dataloader):\n",
    "    opt.learning_rate = config[\"lr\"]\n",
    "    opt.temperature = config[\"temperature\"]\n",
    "    model, criterion = set_model(opt)\n",
    "    optimizer = set_optimizer(opt, model)\n",
    "    print(\"\\n\\nTraining...\")\n",
    "    print(\"Running for {} epochs...\".format(opt.epochs))\n",
    "    best_loss = 0\n",
    "    best_epoch = 1\n",
    "    # training routine\n",
    "    for epoch in range(1, opt.epochs + 1):\n",
    "        adjust_learning_rate(opt, optimizer, epoch)\n",
    "\n",
    "        # train for one epoch\n",
    "        time1 = time.time()\n",
    "        train_loss = train_supcon(supcon_loader[\"train\"], model, criterion, optimizer, epoch, opt)\n",
    "        valid_loss = valid_supcon(supcon_loader[\"val\"], model, criterion, opt)\n",
    "        time2 = time.time()\n",
    "        print(\"epoch {}, total time {:.2f}, train loss: {:.2f}, valid loss: {:.2f}\".format(epoch, time2 - time1, train_loss, 1/valid_loss))\n",
    "\n",
    "        if valid_loss > best_loss:\n",
    "            best_loss = valid_loss\n",
    "            best_model = model\n",
    "            best_optimizer = optimizer\n",
    "            best_epoch = epoch\n",
    "    print(\"\\n\\nTesting..\")\n",
    "    best_model.eval()\n",
    "    targets = []\n",
    "    embeddings = torch.zeros((0, opt.embedding_size), dtype=torch.float32)\n",
    "    for data, label, _ in dataloader[\"train\"]:\n",
    "        data = data.to(DEVICE)\n",
    "        embedding = best_model(data)\n",
    "        targets.extend(label.detach().cpu().tolist())\n",
    "        embeddings = torch.cat((embeddings, embedding.detach().cpu()), dim=0)\n",
    "    x_embed = np.array(embeddings)\n",
    "    y = np.array(targets)\n",
    "\n",
    "    # Create a logistic regression classifier\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(x_embed, y)\n",
    "    predictions = classifier.predict(x_embed)\n",
    "\n",
    "    print(\"\\nResult for Train:\")\n",
    "    train_score, *_ = calc_score(y, predictions, verbose=True, task=int(opt.task_in[-2]))\n",
    "    print(\"\\nResult for Valid:\")\n",
    "    val_score = test_supcon(best_model, classifier, dataloader[\"val\"], opt)\n",
    "    # Here we save a checkpoint. It is automatically registered with\n",
    "    # Ray Tune and can be accessed through `session.get_checkpoint()`\n",
    "    # API in future iterations.\n",
    "    os.makedirs(\"tuning_models\", exist_ok=True)\n",
    "    torch.save(\n",
    "        (best_model.state_dict(), optimizer.state_dict()),\n",
    "        \"tuning_models/checkpoint.pt\",\n",
    "    )\n",
    "    checkpoint = Checkpoint.from_directory(\"tuning_models\")\n",
    "    session.report(\n",
    "        {\n",
    "            \"score\": val_score,\n",
    "        },\n",
    "        checkpoint=checkpoint,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_target, strat_mode = TUNE_STRAT\n",
    "scheduler = ASHAScheduler(max_t=TUNE_MAX_EPOCH, grace_period=1, reduction_factor=2)\n",
    "algo = HyperOptSearch(metric=strat_target, mode=strat_mode)\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_resources(\n",
    "        tune.with_parameters(\n",
    "            trainingLoop, opt=opt, supcon_loader=supcon_loader, dataloader=dataloader\n",
    "        ),\n",
    "        resources={\"cpu\": TUNE_CPU_PER_TRIAL, \"gpu\": TUNE_GPU_PER_TRIAL},\n",
    "    ),\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=strat_target,\n",
    "        mode=strat_mode,\n",
    "        scheduler=scheduler,\n",
    "        num_samples=TUNE_SAMPLE_NUM,\n",
    "        search_alg=algo,\n",
    "    ),\n",
    "    param_space=TUNE_CONFIG,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Home](#table-of-content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = tuner.fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Home](#table-of-content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = results.get_best_result(strat_target, strat_mode)\n",
    "best_val_score = best_result.metrics[\"score\"]\n",
    "# best_val_loss = best_result.metrics[\"loss\"]\n",
    "# best_val_accu = best_result.metrics[\"accuracy\"]\n",
    "print(\"Best trial config: {}\".format(best_result.config))\n",
    "# print(\"Best trial final validation loss: {}\".format(best_val_loss))\n",
    "# print(\"Best trial final validation accuracy: {}\".format(best_val_accu))\n",
    "print(\"Best trial final validation score: {}\".format(best_val_score))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Home](#table-of-content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing using Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test the best Network\n",
    "### Set the opt based on best result ----- Edit this part for different variables\n",
    "opt.temperature = best_result.config[\"temperature\"]\n",
    "opt.learning_rate = best_result.config[\"lr\"]\n",
    "### --------------------------------------------------\n",
    "test_network = mdl.SupConResNet(\n",
    "    name=opt.model, \n",
    "    head=opt.head,\n",
    "    feat_dim=opt.embedding_size,\n",
    "    dropout=opt.dropout,\n",
    ").to(DEVICE)\n",
    "best_chkpt = os.path.join(best_result.checkpoint.to_directory(), \"checkpoint.pt\")\n",
    "\n",
    "model_state, _ = torch.load(best_chkpt)\n",
    "test_network.load_state_dict(model_state)\n",
    "test_network.eval()\n",
    "\n",
    "targets = []\n",
    "embeddings = torch.zeros((0, opt.embedding_size), dtype=torch.float32)\n",
    "for data, label, _ in dataloader[\"train\"]:\n",
    "    data = data.to(DEVICE)\n",
    "    embedding = test_network(data)\n",
    "    targets.extend(label.detach().cpu().tolist())\n",
    "    embeddings = torch.cat((embeddings, embedding.detach().cpu()), dim=0)\n",
    "\n",
    "x_embed = np.array(embeddings)\n",
    "y = np.array(targets)\n",
    "\n",
    "# Create a logistic regression classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(x_embed, y)\n",
    "predictions = classifier.predict(x_embed)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"\\nResult for Intra:\")\n",
    "    intra_score = test_supcon(test_network, classifier, intra_testloader, opt)\n",
    "    print(\"\\nResult for Inter:\")\n",
    "    inter_score = test_supcon(test_network, classifier, inter_testloader, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logMessage = (\n",
    "    f\"SupContrast with: {opt.model}, Task: {TASK_IN}, inter score: {inter_score:>0.3}, \"\n",
    "    f\"intra score: {intra_score:>0.3}, val_score: {best_val_score:>0.3}, best trial config: {best_result.config}\")\n",
    "logger.info(logMessage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECGJH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
